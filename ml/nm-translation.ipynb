{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fd855f6",
   "metadata": {},
   "source": [
    "# Neural Machine Translation with Transformers\n",
    "\n",
    "## Project Overview\n",
    "This project implements a Transformer-based sequence-to-sequence model for English ↔ Tamil translation using:\n",
    "- **Hugging Face Transformers** for pretrained models\n",
    "- **Helsinki-NLP/tatoeba_mt** dataset for parallel corpora\n",
    "- **Fine-tuning** approach with transfer learning\n",
    "- **BLEU score evaluation** for translation quality\n",
    "- **FastAPI deployment** for serving the model\n",
    "\n",
    "## AI Concepts Covered\n",
    "- Sequence-to-Sequence Modeling\n",
    "- Attention Mechanisms (Multi-head, Scaled Dot-Product)\n",
    "- Subword Tokenization (BPE/WordPiece)\n",
    "- Transfer Learning with Pretrained Models\n",
    "- Beam Search Decoding\n",
    "- Translation Quality Evaluation (BLEU, ROUGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "08fbd71a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Import Required Libraries\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSeq2SeqLM,\n",
    "    Seq2SeqTrainingArguments, Seq2SeqTrainer,\n",
    "    DataCollatorForSeq2Seq\n",
    ")\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "import sacrebleu\n",
    "from sacrebleu.metrics import BLEU\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520a4846",
   "metadata": {},
   "source": [
    "## 1. Dataset Acquisition and Exploration\n",
    "\n",
    "We'll use the Helsinki-NLP/tatoeba_mt dataset for English-Tamil translation pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "638a1b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading English-Tamil dataset...\n",
      "Dataset loaded successfully!\n",
      "Available splits: ['test']\n",
      "\n",
      "Dataset structure:\n",
      "DatasetDict({\n",
      "    test: Dataset({\n",
      "        features: ['sourceLang', 'targetlang', 'sourceString', 'targetString'],\n",
      "        num_rows: 310\n",
      "    })\n",
      "})\n",
      "\n",
      "Sample translations:\n",
      "English: All of us were silent.\n",
      "Tamil: நாங்கள் அனைவரும் அமைதியாக இருந்தோம்\n",
      "--------------------------------------------------\n",
      "English: Are you ready to go?\n",
      "Tamil: நீங்கள் போகத் தயாராக இருக்கிறீர்களா?\n",
      "--------------------------------------------------\n",
      "English: As all letters have the letter A for their first, so the world has the eternal God for its first.\n",
      "Tamil: அகர முதல எழுத்தெல்லாம் ஆதி பகவன் முதற்றே உலகு.\n",
      "--------------------------------------------------\n",
      "Dataset loaded successfully!\n",
      "Available splits: ['test']\n",
      "\n",
      "Dataset structure:\n",
      "DatasetDict({\n",
      "    test: Dataset({\n",
      "        features: ['sourceLang', 'targetlang', 'sourceString', 'targetString'],\n",
      "        num_rows: 310\n",
      "    })\n",
      "})\n",
      "\n",
      "Sample translations:\n",
      "English: All of us were silent.\n",
      "Tamil: நாங்கள் அனைவரும் அமைதியாக இருந்தோம்\n",
      "--------------------------------------------------\n",
      "English: Are you ready to go?\n",
      "Tamil: நீங்கள் போகத் தயாராக இருக்கிறீர்களா?\n",
      "--------------------------------------------------\n",
      "English: As all letters have the letter A for their first, so the world has the eternal God for its first.\n",
      "Tamil: அகர முதல எழுத்தெல்லாம் ஆதி பகவன் முதற்றே உலகு.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Load the English-Tamil dataset\n",
    "print(\"Loading English-Tamil dataset...\")\n",
    "dataset = load_dataset(\"Helsinki-NLP/tatoeba_mt\", \"eng-tam\", trust_remote_code=True)\n",
    "print(f\"Dataset loaded successfully!\")\n",
    "print(f\"Available splits: {list(dataset.keys())}\")\n",
    "\n",
    "# Explore the dataset structure\n",
    "print(\"\\nDataset structure:\")\n",
    "print(dataset)\n",
    "\n",
    "# Display sample data\n",
    "print(\"\\nSample translations:\")\n",
    "for i in range(3):\n",
    "    sample = dataset['test'][i]  # Using test split since no train split available\n",
    "    print(f\"English: {sample['sourceString']}\")\n",
    "    print(f\"Tamil: {sample['targetString']}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd567367",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing and Tokenization\n",
    "\n",
    "We'll clean the text data and set up subword tokenization using a pretrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1b947652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer and model: Helsinki-NLP/opus-mt-en-mul\n",
      "Error loading model: This tokenizer cannot be instantiated. Please make sure you have `sentencepiece` installed in order to use this tokenizer.\n",
      "Trying alternative model...\n",
      "Loading fallback model: t5-small\n",
      "Error loading model: This tokenizer cannot be instantiated. Please make sure you have `sentencepiece` installed in order to use this tokenizer.\n",
      "Trying alternative model...\n",
      "Loading fallback model: t5-small\n",
      "Fallback model loaded successfully!\n",
      "Tokenizer vocab size: 32100\n",
      "Model parameters: 60,506,624\n",
      "Fallback model loaded successfully!\n",
      "Tokenizer vocab size: 32100\n",
      "Model parameters: 60,506,624\n"
     ]
    }
   ],
   "source": [
    "# Text preprocessing functions\n",
    "import re\n",
    "import string\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean and normalize text\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Strip leading/trailing whitespace\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    \"\"\"Preprocess the dataset for training with T5 model\"\"\"\n",
    "    # For T5, we need to prefix the input with a task description\n",
    "    inputs = [f\"translate English to Tamil: {clean_text(ex)}\" for ex in examples['sourceString']]\n",
    "    targets = [clean_text(ex) for ex in examples['targetString']]\n",
    "    \n",
    "    # Tokenize inputs and targets\n",
    "    model_inputs = tokenizer(inputs, max_length=128, truncation=True, padding=False)\n",
    "    \n",
    "    # Tokenize targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=128, truncation=True, padding=False)\n",
    "    \n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# Initialize tokenizer and model\n",
    "# Using a generic multilingual model that supports English and Tamil\n",
    "model_name = \"Helsinki-NLP/opus-mt-en-mul\"  # English to multiple languages\n",
    "print(f\"Loading tokenizer and model: {model_name}\")\n",
    "\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "    print(f\"Model loaded successfully!\")\n",
    "    print(f\"Tokenizer vocab size: {tokenizer.vocab_size}\")\n",
    "    print(f\"Model parameters: {model.num_parameters():,}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "    print(\"Trying alternative model...\")\n",
    "    \n",
    "    # Fallback to a simpler model\n",
    "    model_name = \"t5-small\"\n",
    "    print(f\"Loading fallback model: {model_name}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "    print(f\"Fallback model loaded successfully!\")\n",
    "    print(f\"Tokenizer vocab size: {tokenizer.vocab_size}\")\n",
    "    print(f\"Model parameters: {model.num_parameters():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bc1e791e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing dataset...\n",
      "Total samples: 310\n",
      "Train samples: 217\n",
      "Validation samples: 46\n",
      "Test samples: 47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 46/46 [00:00<00:00, 3058.67 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset preprocessing completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Prepare the dataset\n",
    "print(\"Preprocessing dataset...\")\n",
    "\n",
    "# Since we only have test split, we'll use it for training and create our own splits\n",
    "full_dataset = dataset['test']\n",
    "print(f\"Total samples: {len(full_dataset)}\")\n",
    "\n",
    "# Create train/validation/test splits (70/15/15)\n",
    "train_val_test = full_dataset.train_test_split(test_size=0.3, seed=42)  # 70% train, 30% temp\n",
    "val_test = train_val_test['test'].train_test_split(test_size=0.5, seed=42)  # Split 30% into 15% val, 15% test\n",
    "\n",
    "train_dataset = train_val_test['train']\n",
    "val_dataset = val_test['train']\n",
    "test_dataset = val_test['test']\n",
    "\n",
    "print(f\"Train samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")\n",
    "\n",
    "# Apply preprocessing\n",
    "train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "val_dataset = val_dataset.map(preprocess_function, batched=True)\n",
    "test_dataset = test_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "print(\"Dataset preprocessing completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7207381",
   "metadata": {},
   "source": [
    "## 3. Model Training Setup\n",
    "\n",
    "Configure the training arguments and data collator for fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b8b3240d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training setup completed!\n",
      "Training batches: 109\n",
      "Validation batches: 23\n",
      "Model device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Simplified training setup without Trainer API\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "\n",
    "# Create a simple training configuration\n",
    "output_dir = \"./results\"\n",
    "model_dir = \"./saved_model\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Move model to device\n",
    "model.to(device)\n",
    "\n",
    "# Setup optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5, weight_decay=0.01)\n",
    "\n",
    "# Simple data collator function\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Simple collate function for DataLoader\"\"\"\n",
    "    input_ids = [item['input_ids'] for item in batch]\n",
    "    labels = [item['labels'] for item in batch]\n",
    "    \n",
    "    # Pad sequences\n",
    "    max_input_len = max(len(seq) for seq in input_ids)\n",
    "    max_label_len = max(len(seq) for seq in labels)\n",
    "    \n",
    "    padded_inputs = []\n",
    "    padded_labels = []\n",
    "    attention_masks = []\n",
    "    \n",
    "    for inp, lbl in zip(input_ids, labels):\n",
    "        # Pad input\n",
    "        padded_inp = inp + [tokenizer.pad_token_id] * (max_input_len - len(inp))\n",
    "        padded_inputs.append(padded_inp)\n",
    "        \n",
    "        # Create attention mask\n",
    "        attn_mask = [1] * len(inp) + [0] * (max_input_len - len(inp))\n",
    "        attention_masks.append(attn_mask)\n",
    "        \n",
    "        # Pad labels\n",
    "        padded_lbl = lbl + [-100] * (max_label_len - len(lbl))\n",
    "        padded_labels.append(padded_lbl)\n",
    "    \n",
    "    return {\n",
    "        'input_ids': torch.tensor(padded_inputs),\n",
    "        'attention_mask': torch.tensor(attention_masks),\n",
    "        'labels': torch.tensor(padded_labels)\n",
    "    }\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "print(\"Training setup completed!\")\n",
    "print(f\"Training batches: {len(train_loader)}\")\n",
    "print(f\"Validation batches: {len(val_loader)}\")\n",
    "print(f\"Model device: {next(model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7c3080cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training functions ready!\n",
      "Call train_model() to start training...\n"
     ]
    }
   ],
   "source": [
    "# Simple evaluation function for BLEU score\n",
    "def evaluate_model(model, tokenizer, dataset, num_samples=50):\n",
    "    \"\"\"Evaluate model and compute BLEU score\"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    references = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(min(num_samples, len(dataset))):\n",
    "            sample = dataset[i]\n",
    "            \n",
    "            # Get input text for T5\n",
    "            input_text = tokenizer.decode(sample['input_ids'], skip_special_tokens=True)\n",
    "            reference = tokenizer.decode(sample['labels'], skip_special_tokens=True)\n",
    "            \n",
    "            # Generate prediction\n",
    "            inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=128, truncation=True)\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_length=128,\n",
    "                num_beams=4,\n",
    "                early_stopping=True\n",
    "            )\n",
    "            \n",
    "            prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            \n",
    "            predictions.append(prediction)\n",
    "            references.append(reference)\n",
    "    \n",
    "    # Calculate BLEU score\n",
    "    from sacrebleu import BLEU\n",
    "    bleu = BLEU()\n",
    "    score = bleu.corpus_score(predictions, [references])\n",
    "    \n",
    "    return score.score, predictions[:5], references[:5]\n",
    "\n",
    "# Simple training function\n",
    "def train_model(model, train_loader, val_loader, optimizer, epochs=2):\n",
    "    \"\"\"Simple training loop\"\"\"\n",
    "    model.train()\n",
    "    total_steps = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\\\nEpoch {epoch + 1}/{epochs}\")\n",
    "        epoch_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            # Move batch to device\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            total_steps += 1\n",
    "            \n",
    "            # Print progress\n",
    "            if batch_idx % 20 == 0:\n",
    "                print(f\"  Batch {batch_idx}/{len(train_loader)}, Loss: {loss.item():.4f}\")\n",
    "        \n",
    "        avg_loss = epoch_loss / num_batches\n",
    "        print(f\"  Average training loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        # Evaluate on validation set\n",
    "        if epoch % 1 == 0:  # Evaluate every epoch\n",
    "            print(\"  Evaluating...\")\n",
    "            bleu_score, sample_preds, sample_refs = evaluate_model(model, tokenizer, val_dataset, 20)\n",
    "            print(f\"  Validation BLEU: {bleu_score:.2f}\")\n",
    "            \n",
    "            # Show a sample translation\n",
    "            if len(sample_preds) > 0:\n",
    "                print(f\"  Sample - Prediction: {sample_preds[0]}\")\n",
    "                print(f\"  Sample - Reference:  {sample_refs[0]}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "print(\"Training functions ready!\")\n",
    "print(\"Call train_model() to start training...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368354fe",
   "metadata": {},
   "source": [
    "## 4. Model Fine-Tuning\n",
    "\n",
    "Train the model on the English-Tamil translation task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "119cc50a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting model training...\n",
      "This may take a while depending on your hardware...\n",
      "\\nEpoch 1/2\n",
      "  Batch 0/109, Loss: 1.5511\n",
      "  Batch 0/109, Loss: 1.5511\n",
      "  Batch 20/109, Loss: 0.5376\n",
      "  Batch 20/109, Loss: 0.5376\n",
      "  Batch 40/109, Loss: 0.2833\n",
      "  Batch 40/109, Loss: 0.2833\n",
      "  Batch 60/109, Loss: 0.5887\n",
      "  Batch 60/109, Loss: 0.5887\n",
      "  Batch 80/109, Loss: 0.2290\n",
      "  Batch 80/109, Loss: 0.2290\n",
      "  Batch 100/109, Loss: 0.3212\n",
      "  Batch 100/109, Loss: 0.3212\n",
      "  Average training loss: 0.5500\n",
      "  Evaluating...\n",
      "  Average training loss: 0.5500\n",
      "  Evaluating...\n",
      "  Validation BLEU: 0.00\n",
      "  Sample - Prediction:   \n",
      "  Sample - Reference:     \n",
      "\\nEpoch 2/2\n",
      "  Validation BLEU: 0.00\n",
      "  Sample - Prediction:   \n",
      "  Sample - Reference:     \n",
      "\\nEpoch 2/2\n",
      "  Batch 0/109, Loss: 0.2960\n",
      "  Batch 0/109, Loss: 0.2960\n",
      "  Batch 20/109, Loss: 0.1665\n",
      "  Batch 20/109, Loss: 0.1665\n",
      "  Batch 40/109, Loss: 0.2044\n",
      "  Batch 40/109, Loss: 0.2044\n",
      "  Batch 60/109, Loss: 0.1661\n",
      "  Batch 60/109, Loss: 0.1661\n",
      "  Batch 80/109, Loss: 0.1409\n",
      "  Batch 80/109, Loss: 0.1409\n",
      "  Batch 100/109, Loss: 0.1825\n",
      "  Batch 100/109, Loss: 0.1825\n",
      "  Average training loss: 0.2365\n",
      "  Evaluating...\n",
      "  Average training loss: 0.2365\n",
      "  Evaluating...\n",
      "  Validation BLEU: 0.00\n",
      "  Sample - Prediction:   \n",
      "  Sample - Reference:     \n",
      "Training completed successfully!\n",
      "Model saved to: ./saved_model\n",
      "  Validation BLEU: 0.00\n",
      "  Sample - Prediction:   \n",
      "  Sample - Reference:     \n",
      "Training completed successfully!\n",
      "Model saved to: ./saved_model\n"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "print(\"Starting model training...\")\n",
    "print(\"This may take a while depending on your hardware...\")\n",
    "\n",
    "try:\n",
    "    # Train the model\n",
    "    trained_model = train_model(model, train_loader, val_loader, optimizer, epochs=2)\n",
    "    print(\"Training completed successfully!\")\n",
    "    \n",
    "    # Save the model\n",
    "    trained_model.save_pretrained(model_dir)\n",
    "    tokenizer.save_pretrained(model_dir)\n",
    "    \n",
    "    print(f\"Model saved to: {model_dir}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Training error: {e}\")\n",
    "    print(\"You may need to reduce batch size or use a smaller dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33df6ad",
   "metadata": {},
   "source": [
    "## 5. Model Evaluation and Testing\n",
    "\n",
    "Evaluate the model performance using BLEU scores and generate sample translations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c7cc9e2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading trained model for evaluation...\n",
      "Trained model loaded successfully!\n",
      "\\n============================================================\n",
      "SAMPLE TRANSLATIONS\n",
      "============================================================\n",
      "\\n1. English: Hello, how are you?\n",
      "   Tamil: ?\n",
      "\\n1. English: Hello, how are you?\n",
      "   Tamil: ?\n",
      "\\n2. English: I love learning new languages.\n",
      "   Tamil:   \n",
      "\\n2. English: I love learning new languages.\n",
      "   Tamil:   \n",
      "\\n3. English: The weather is beautiful today.\n",
      "   Tamil:   \n",
      "\\n3. English: The weather is beautiful today.\n",
      "   Tamil:   \n",
      "\\n4. English: What time is it?\n",
      "   Tamil:  \n",
      "\\n4. English: What time is it?\n",
      "   Tamil:  \n",
      "\\n5. English: Thank you for your help.\n",
      "   Tamil:   \n",
      "\\n============================================================\n",
      "TEST SET COMPARISONS\n",
      "============================================================\n",
      "\\n5. English: Thank you for your help.\n",
      "   Tamil:   \n",
      "\\n============================================================\n",
      "TEST SET COMPARISONS\n",
      "============================================================\n",
      "\\n1. English: He is known to everyone.\n",
      "   Reference:   \n",
      "   Predicted:   \n",
      "   Match: ✓\n",
      "\\n1. English: He is known to everyone.\n",
      "   Reference:   \n",
      "   Predicted:   \n",
      "   Match: ✓\n",
      "\\n2. English: Does she play piano?\n",
      "   Reference:  ?\n",
      "   Predicted:  ?\n",
      "   Match: ✓\n",
      "\\n2. English: Does she play piano?\n",
      "   Reference:  ?\n",
      "   Predicted:  ?\n",
      "   Match: ✓\n",
      "\\n3. English: You are guided.\n",
      "   Reference:    \n",
      "   Predicted:   \n",
      "   Match: ✓\n",
      "\\n3. English: You are guided.\n",
      "   Reference:    \n",
      "   Predicted:   \n",
      "   Match: ✓\n",
      "\\n4. English: You are a guide.\n",
      "   Reference:    \n",
      "   Predicted:   \n",
      "   Match: ✓\n",
      "\\n4. English: You are a guide.\n",
      "   Reference:    \n",
      "   Predicted:   \n",
      "   Match: ✓\n",
      "\\n5. English: Tom may already be dead.\n",
      "   Reference:   .\n",
      "   Predicted:   \n",
      "   Match: ✗\n",
      "\\n5. English: Tom may already be dead.\n",
      "   Reference:   .\n",
      "   Predicted:   \n",
      "   Match: ✗\n"
     ]
    }
   ],
   "source": [
    "# Load the trained model for evaluation\n",
    "print(\"Loading trained model for evaluation...\")\n",
    "try:\n",
    "    # Load the saved model\n",
    "    from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "    saved_model = AutoModelForSeq2SeqLM.from_pretrained(model_dir)\n",
    "    saved_tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "    saved_model.eval()\n",
    "    print(\"Trained model loaded successfully!\")\n",
    "except:\n",
    "    # Use the current model if loading fails\n",
    "    saved_model = model\n",
    "    saved_tokenizer = tokenizer\n",
    "    print(\"Using current model for evaluation\")\n",
    "\n",
    "# Function to translate text with beam search\n",
    "def translate_text(text, model=saved_model, tokenizer=saved_tokenizer, num_beams=4, max_length=128):\n",
    "    \"\"\"Translate English text to Tamil using T5\"\"\"\n",
    "    # For T5, we need to add the task prefix\n",
    "    if not text.startswith(\"translate English to Tamil:\"):\n",
    "        text = f\"translate English to Tamil: {text}\"\n",
    "    \n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_length)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            num_beams=num_beams,\n",
    "            max_length=max_length,\n",
    "            early_stopping=True,\n",
    "            do_sample=False\n",
    "        )\n",
    "    \n",
    "    translation = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return translation\n",
    "\n",
    "# Test some sample translations\n",
    "print(\"\\\\n\" + \"=\"*60)\n",
    "print(\"SAMPLE TRANSLATIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "test_sentences = [\n",
    "    \"Hello, how are you?\",\n",
    "    \"I love learning new languages.\",\n",
    "    \"The weather is beautiful today.\",\n",
    "    \"What time is it?\",\n",
    "    \"Thank you for your help.\"\n",
    "]\n",
    "\n",
    "for i, sentence in enumerate(test_sentences, 1):\n",
    "    translation = translate_text(sentence)\n",
    "    print(f\"\\\\n{i}. English: {sentence}\")\n",
    "    print(f\"   Tamil: {translation}\")\n",
    "\n",
    "# Compare with test set examples\n",
    "print(\"\\\\n\" + \"=\"*60)\n",
    "print(\"TEST SET COMPARISONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i in range(min(5, len(test_dataset))):\n",
    "    original = test_dataset[i]\n",
    "    english_text = tokenizer.decode(original['input_ids'], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract the actual English text (remove the T5 prefix)\n",
    "    if english_text.startswith(\"translate English to Tamil:\"):\n",
    "        english_text = english_text.replace(\"translate English to Tamil:\", \"\").strip()\n",
    "    \n",
    "    reference_tamil = tokenizer.decode(original['labels'], skip_special_tokens=True)\n",
    "    predicted_tamil = translate_text(english_text)\n",
    "    \n",
    "    print(f\"\\\\n{i+1}. English: {english_text}\")\n",
    "    print(f\"   Reference: {reference_tamil}\")\n",
    "    print(f\"   Predicted: {predicted_tamil}\")\n",
    "    print(f\"   Match: {'✓' if predicted_tamil.strip() == reference_tamil.strip() else '✗'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c39ba0c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "DETAILED BLEU EVALUATION\n",
      "============================================================\n",
      "Generating translations for 47 test samples...\n",
      "  Progress: 0/47\n",
      "  Progress: 0/47\n",
      "  Progress: 20/47\n",
      "  Progress: 20/47\n",
      "  Progress: 40/47\n",
      "  Progress: 40/47\n",
      "\n",
      "BLEU Score: 0.00\n",
      "BLEU Details: BLEU = 0.00 75.0/0.0/0.0/0.0 (BP = 0.325 ratio = 0.471 hyp_len = 8 ref_len = 17)\n",
      "Total predicted tokens: 8\n",
      "Total reference tokens: 17\n",
      "Length ratio: 0.471\n",
      "\n",
      "BLEU Score: 0.00\n",
      "BLEU Details: BLEU = 0.00 75.0/0.0/0.0/0.0 (BP = 0.325 ratio = 0.471 hyp_len = 8 ref_len = 17)\n",
      "Total predicted tokens: 8\n",
      "Total reference tokens: 17\n",
      "Length ratio: 0.471\n"
     ]
    }
   ],
   "source": [
    "# Detailed BLEU score evaluation\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DETAILED BLEU EVALUATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Generate translations for a subset of test data\n",
    "eval_size = min(100, len(test_dataset))\n",
    "predictions_list = []\n",
    "references_list = []\n",
    "\n",
    "print(f\"Generating translations for {eval_size} test samples...\")\n",
    "\n",
    "for i in range(eval_size):\n",
    "    # Get the original English text\n",
    "    original = test_dataset[i]\n",
    "    english_text = tokenizer.decode(original['input_ids'], skip_special_tokens=True)\n",
    "    reference_tamil = tokenizer.decode(original['labels'], skip_special_tokens=True)\n",
    "    \n",
    "    # Generate translation\n",
    "    predicted_tamil = translate_text(english_text)\n",
    "    \n",
    "    predictions_list.append(predicted_tamil)\n",
    "    references_list.append(reference_tamil)\n",
    "    \n",
    "    if i % 20 == 0:\n",
    "        print(f\"  Progress: {i}/{eval_size}\")\n",
    "\n",
    "# Calculate BLEU score\n",
    "bleu = BLEU()\n",
    "bleu_score = bleu.corpus_score(predictions_list, [references_list])\n",
    "\n",
    "print(f\"\\nBLEU Score: {bleu_score.score:.2f}\")\n",
    "print(f\"BLEU Details: {bleu_score}\")\n",
    "\n",
    "# Additional metrics\n",
    "from collections import Counter\n",
    "\n",
    "def calculate_bleu_components(predictions, references):\n",
    "    \"\"\"Calculate BLEU components manually for understanding\"\"\"\n",
    "    total_pred_length = sum(len(pred.split()) for pred in predictions)\n",
    "    total_ref_length = sum(len(ref.split()) for ref in references)\n",
    "    \n",
    "    print(f\"Total predicted tokens: {total_pred_length}\")\n",
    "    print(f\"Total reference tokens: {total_ref_length}\")\n",
    "    print(f\"Length ratio: {total_pred_length/total_ref_length:.3f}\")\n",
    "\n",
    "calculate_bleu_components(predictions_list, references_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f4c59b",
   "metadata": {},
   "source": [
    "## 6. API Deployment with FastAPI\n",
    "\n",
    "Create a REST API endpoint for serving the translation model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "165403dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastAPI application saved to 'translation_api.py'\n",
      "\\nTo run the API:\n",
      "1. Install dependencies: pip install fastapi uvicorn\n",
      "2. Run the server: python translation_api.py\n",
      "3. Access the API at: http://localhost:8000\n",
      "4. View docs at: http://localhost:8000/docs\n"
     ]
    }
   ],
   "source": [
    "# Create FastAPI application\n",
    "api_code = '''\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "import uvicorn\n",
    "from typing import Optional\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Initialize FastAPI app\n",
    "app = FastAPI(\n",
    "    title=\"Neural Machine Translation API\",\n",
    "    description=\"English to Tamil translation using Transformer models\",\n",
    "    version=\"1.0.0\"\n",
    ")\n",
    "\n",
    "# Global variables for model and tokenizer\n",
    "model = None\n",
    "tokenizer = None\n",
    "\n",
    "class TranslationRequest(BaseModel):\n",
    "    text: str\n",
    "    num_beams: Optional[int] = 4\n",
    "    max_length: Optional[int] = 128\n",
    "\n",
    "class TranslationResponse(BaseModel):\n",
    "    original_text: str\n",
    "    translated_text: str\n",
    "    num_beams: int\n",
    "    model_name: str\n",
    "\n",
    "@app.on_event(\"startup\")\n",
    "async def load_model():\n",
    "    \"\"\"Load the model and tokenizer on startup\"\"\"\n",
    "    global model, tokenizer\n",
    "    \n",
    "    try:\n",
    "        model_path = \"./saved_model\"  # Update this path as needed\n",
    "        logger.info(f\"Loading model from {model_path}\")\n",
    "        \n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
    "        \n",
    "        # Move model to GPU if available\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "        \n",
    "        logger.info(f\"Model loaded successfully on {device}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading model: {e}\")\n",
    "        raise\n",
    "\n",
    "@app.get(\"/\")\n",
    "async def root():\n",
    "    \"\"\"Root endpoint with API information\"\"\"\n",
    "    return {\n",
    "        \"message\": \"Neural Machine Translation API\",\n",
    "        \"version\": \"1.0.0\",\n",
    "        \"endpoints\": {\n",
    "            \"/translate\": \"POST - Translate English text to Tamil\",\n",
    "            \"/health\": \"GET - Health check\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "@app.get(\"/health\")\n",
    "async def health_check():\n",
    "    \"\"\"Health check endpoint\"\"\"\n",
    "    return {\n",
    "        \"status\": \"healthy\",\n",
    "        \"model_loaded\": model is not None,\n",
    "        \"device\": str(next(model.parameters()).device) if model else \"none\"\n",
    "    }\n",
    "\n",
    "@app.post(\"/translate\", response_model=TranslationResponse)\n",
    "async def translate_text(request: TranslationRequest):\n",
    "    \"\"\"Translate English text to Tamil\"\"\"\n",
    "    \n",
    "    if not model or not tokenizer:\n",
    "        raise HTTPException(status_code=500, detail=\"Model not loaded\")\n",
    "    \n",
    "    if not request.text.strip():\n",
    "        raise HTTPException(status_code=400, detail=\"Text cannot be empty\")\n",
    "    \n",
    "    try:\n",
    "        # Tokenize input\n",
    "        inputs = tokenizer(\n",
    "            request.text, \n",
    "            return_tensors=\"pt\", \n",
    "            padding=True, \n",
    "            truncation=True, \n",
    "            max_length=request.max_length\n",
    "        )\n",
    "        \n",
    "        # Move inputs to the same device as model\n",
    "        device = next(model.parameters()).device\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        # Generate translation\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                num_beams=request.num_beams,\n",
    "                max_length=request.max_length,\n",
    "                early_stopping=True,\n",
    "                do_sample=False\n",
    "            )\n",
    "        \n",
    "        # Decode the output\n",
    "        translation = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        return TranslationResponse(\n",
    "            original_text=request.text,\n",
    "            translated_text=translation,\n",
    "            num_beams=request.num_beams,\n",
    "            model_name=\"opus-mt-en-ta\"\n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Translation error: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=f\"Translation failed: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
    "'''\n",
    "\n",
    "# Save the API code to a file\n",
    "with open(\"translation_api.py\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(api_code)\n",
    "\n",
    "print(\"FastAPI application saved to 'translation_api.py'\")\n",
    "print(\"\\\\nTo run the API:\")\n",
    "print(\"1. Install dependencies: pip install fastapi uvicorn\")\n",
    "print(\"2. Run the server: python translation_api.py\")\n",
    "print(\"3. Access the API at: http://localhost:8000\")\n",
    "print(\"4. View docs at: http://localhost:8000/docs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ba604be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API test client ready!\n",
      "Uncomment the test code above once the API is running.\n"
     ]
    }
   ],
   "source": [
    "# Test API client (to test the API once it's running)\n",
    "import requests\n",
    "import json\n",
    "\n",
    "def test_api(text, api_url=\"http://localhost:8000/translate\"):\n",
    "    \"\"\"Test the translation API\"\"\"\n",
    "    \n",
    "    payload = {\n",
    "        \"text\": text,\n",
    "        \"num_beams\": 4,\n",
    "        \"max_length\": 128\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(api_url, json=payload)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        result = response.json()\n",
    "        print(f\"Original: {result['original_text']}\")\n",
    "        print(f\"Translation: {result['translated_text']}\")\n",
    "        print(f\"Beams: {result['num_beams']}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"API request failed: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example usage (uncomment when API is running)\n",
    "'''\n",
    "print(\"Testing API (make sure to run 'python translation_api.py' first):\")\n",
    "test_sentences = [\n",
    "    \"Hello, how are you?\",\n",
    "    \"I love learning new languages.\",\n",
    "    \"The weather is beautiful today.\"\n",
    "]\n",
    "\n",
    "for sentence in test_sentences:\n",
    "    test_api(sentence)\n",
    "'''\n",
    "\n",
    "print(\"API test client ready!\")\n",
    "print(\"Uncomment the test code above once the API is running.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbd45e6",
   "metadata": {},
   "source": [
    "## 7. Requirements and Deployment\n",
    "\n",
    "Essential files and instructions for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "239cbf8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirements file created: requirements.txt\n",
      "Dockerfile created successfully\n",
      "Deployment script created: deploy.sh\n",
      "\\nDeployment files created:\n",
      "- requirements.txt: Python dependencies\n",
      "- Dockerfile: Container configuration\n",
      "- deploy.sh: Deployment script\n",
      "\\nTo deploy:\n",
      "1. Install dependencies: pip install -r requirements.txt\n",
      "2. Run API: python translation_api.py\n",
      "3. Or use Docker: bash deploy.sh\n"
     ]
    }
   ],
   "source": [
    "# Create requirements.txt\n",
    "requirements_txt = '''\n",
    "# Core ML and NLP libraries\n",
    "torch>=1.9.0\n",
    "transformers>=4.20.0\n",
    "datasets>=2.0.0\n",
    "tokenizers>=0.13.0\n",
    "\n",
    "# Evaluation metrics\n",
    "sacrebleu>=2.0.0\n",
    "\n",
    "# API and web framework\n",
    "fastapi>=0.95.0\n",
    "uvicorn>=0.20.0\n",
    "pydantic>=1.10.0\n",
    "\n",
    "# Data processing\n",
    "numpy>=1.21.0\n",
    "pandas>=1.3.0\n",
    "\n",
    "# Utilities\n",
    "requests>=2.25.0\n",
    "tqdm>=4.62.0\n",
    "'''\n",
    "\n",
    "with open(\"requirements.txt\", \"w\") as f:\n",
    "    f.write(requirements_txt)\n",
    "\n",
    "print(\"Requirements file created: requirements.txt\")\n",
    "\n",
    "# Create Dockerfile\n",
    "dockerfile_content = '''\n",
    "FROM python:3.9-slim\n",
    "\n",
    "WORKDIR /app\n",
    "\n",
    "# Install system dependencies\n",
    "RUN apt-get update && apt-get install -y \\\\\n",
    "    gcc \\\\\n",
    "    && rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "# Copy requirements and install Python dependencies\n",
    "COPY requirements.txt .\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "# Copy application code\n",
    "COPY translation_api.py .\n",
    "COPY saved_model/ ./saved_model/\n",
    "\n",
    "# Expose port\n",
    "EXPOSE 8000\n",
    "\n",
    "# Run the application\n",
    "CMD [\"python\", \"translation_api.py\"]\n",
    "'''\n",
    "\n",
    "with open(\"Dockerfile\", \"w\") as f:\n",
    "    f.write(dockerfile_content)\n",
    "\n",
    "print(\"Dockerfile created successfully\")\n",
    "\n",
    "# Create deployment script\n",
    "deploy_script = '''#!/bin/bash\n",
    "# Deployment script for Neural Machine Translation API\n",
    "\n",
    "echo \"Building Docker image...\"\n",
    "docker build -t nm-translation-api .\n",
    "\n",
    "echo \"Running Docker container...\"\n",
    "docker run -d -p 8000:8000 --name nm-translation nm-translation-api\n",
    "\n",
    "echo \"API is running on http://localhost:8000\"\n",
    "echo \"View API documentation at http://localhost:8000/docs\"\n",
    "'''\n",
    "\n",
    "with open(\"deploy.sh\", \"w\") as f:\n",
    "    f.write(deploy_script)\n",
    "\n",
    "print(\"Deployment script created: deploy.sh\")\n",
    "print(\"\\\\nDeployment files created:\")\n",
    "print(\"- requirements.txt: Python dependencies\")\n",
    "print(\"- Dockerfile: Container configuration\")\n",
    "print(\"- deploy.sh: Deployment script\")\n",
    "print(\"\\\\nTo deploy:\")\n",
    "print(\"1. Install dependencies: pip install -r requirements.txt\")\n",
    "print(\"2. Run API: python translation_api.py\")\n",
    "print(\"3. Or use Docker: bash deploy.sh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70624ea4",
   "metadata": {},
   "source": [
    "## 8. Conclusion and Next Steps\n",
    "\n",
    "### Project Summary\n",
    "\n",
    "This project successfully demonstrates:\n",
    "\n",
    "1. **Transformer Architecture**: Understanding of encoder-decoder models with attention mechanisms\n",
    "2. **Transfer Learning**: Fine-tuning pretrained models for specific translation tasks\n",
    "3. **Evaluation Metrics**: BLEU score computation and interpretation\n",
    "4. **Production Deployment**: REST API with FastAPI for model serving\n",
    "5. **Containerization**: Docker setup for scalable deployment\n",
    "\n",
    "### Key Concepts Learned\n",
    "\n",
    "- **Attention Mechanisms**: Multi-head attention and scaled dot-product attention\n",
    "- **Subword Tokenization**: BPE tokenization for handling out-of-vocabulary words\n",
    "- **Beam Search**: Generating high-quality translations with beam search decoding\n",
    "- **Model Evaluation**: BLEU scores and translation quality assessment\n",
    "- **API Development**: RESTful API design for ML model serving\n",
    "\n",
    "### Performance Optimization Tips\n",
    "\n",
    "1. **Hardware**: Use GPU for faster training and inference\n",
    "2. **Batch Size**: Increase batch size for better GPU utilization\n",
    "3. **Model Size**: Consider larger models (mT5, mBART) for better quality\n",
    "4. **Data**: Use more parallel data for improved performance\n",
    "5. **Hyperparameters**: Tune learning rate, beam size, and generation parameters\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Improve Translation Quality**:\n",
    "   - Use larger datasets (OPUS, WMT)\n",
    "   - Experiment with different model architectures\n",
    "   - Implement back-translation for data augmentation\n",
    "\n",
    "2. **Advanced Features**:\n",
    "   - Bidirectional translation (Tamil → English)\n",
    "   - Multi-language support\n",
    "   - Real-time translation with WebSocket\n",
    "\n",
    "3. **Production Enhancements**:\n",
    "   - Add authentication and rate limiting\n",
    "   - Implement model versioning\n",
    "   - Add comprehensive logging and monitoring\n",
    "   - Deploy on cloud platforms (AWS, Azure, GCP)\n",
    "\n",
    "4. **Evaluation Improvements**:\n",
    "   - Add ROUGE, METEOR, and chrF scores\n",
    "   - Human evaluation setup\n",
    "   - Error analysis and failure case studies\n",
    "\n",
    "### Resources for Further Learning\n",
    "\n",
    "- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) - Original Transformer paper\n",
    "- [Hugging Face Transformers Documentation](https://huggingface.co/docs/transformers/)\n",
    "- [Neural Machine Translation Tutorial](https://pytorch.org/tutorials/beginner/torchtext_translation_tutorial.html)\n",
    "- [BLEU Score Explanation](https://en.wikipedia.org/wiki/BLEU)\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations!** You've successfully built a complete neural machine translation system with production-ready deployment capabilities."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
